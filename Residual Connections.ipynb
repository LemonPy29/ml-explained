{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will study a not so new technique, or arquitecture if you want, to train neuronal networks, specially the deep ones. It's called residual connections, and was introduced in\n",
    "this [paper](https://arxiv.org/abs/1512.03385). The main motivation behind was the question why deeper networks get worse results than the shorter ones. And if you're thinking it's due to overfitting, I'm talking about worse train loss, so the reason is other. I've not seen yet a formal argument of why this could happen, but it's probably a vanishing-gradient problem. \n",
    "\n",
    "In a few words, a residual connection gives the posibility to the net to *skip* one or more layers. The implementation is very simple. Just add the output of one or more of blocks to the input of those blocks, so theorically, the network can learn whether if a block is useful or not and open the posibility to potential add a lots and lots of layers.\n",
    "\n",
    "Additionally, we'll see how a network that take an alternative path with differents convs compare to normal nets and resnets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with the CIFAR10 dataset. I stole this import from the [pytorch tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3c1ce66cae4e56947eb45b4af0817d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "bs = 128\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start defining the clasical conv/batchnorm/relu block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self,c_in,c_out,fs,p=0,rl = True):\n",
    "        super().__init__()\n",
    "        self.c_in, self.c_out, self.fs, self.rl = c_in, c_out, fs, rl\n",
    "        self.conv = nn.Conv2d(self.c_in,self.c_out,self.fs,padding=p)\n",
    "        self.norm, self.relu = nn.BatchNorm2d(self.c_out), nn.ReLU()\n",
    "        \n",
    "    def forward(self,x): \n",
    "        return self.relu(self.norm(self.conv(x))) if self.rl else self.norm(self.conv(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this cell we implement a basic ResBlock. In order to keep the channels of the same size we need to pad it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,nc,fs):\n",
    "        super().__init__()\n",
    "        self.nc, self.fs = nc, fs\n",
    "        self.a = Block(self.nc,self.nc,self.fs,p=1)\n",
    "        self.b = Block(self.nc,self.nc,self.fs,rl=None,p=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.a(self.b(x))\n",
    "        return self.relu(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in this one two differents outputs are added. One it's the result of two equals consecutive blocks and the other it's the output of just one with a bigger kernel size. You have to choose the filter sizes carefully to have outputs with the same dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AltBlock(nn.Module):\n",
    "    def __init__(self,nc,fs1,fs2):\n",
    "        super().__init__()\n",
    "        self.nc, self.fs1, self.fs2 = nc, fs1, fs2\n",
    "        self.a = Block(self.nc,self.nc,self.fs1)\n",
    "        self.b = Block(self.nc,self.nc,self.fs1, rl = None)\n",
    "        self.c = Block(self.nc,self.nc,self.fs2, rl = None)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.a(self.b(x))\n",
    "        z = self.c(x)\n",
    "        return self.relu(y+z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we build our nets. There will be four of them:\n",
    "* The first one is just a regular net. Has several blocks with the same filter size of three and the number of channels is duplicated every two blocks.\n",
    "* The second it's a shorter net with bigger filter sizes.\n",
    "* The third one altern regular blocks with Resblocks\n",
    "* The forth one altern regular blocks with Altblocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net0 = nn.Sequential(Block(3,6,5),\n",
    "                     Block(6,16,3),\n",
    "                     Block(16,16,3),\n",
    "                     Block(16,16,3),\n",
    "                     Block(16,32,3),\n",
    "                     Block(32,32,3),\n",
    "                     Block(32,32,3),\n",
    "                     Block(32,64,3),\n",
    "                     Block(64,64,3),\n",
    "                     Block(64,64,3),\n",
    "                     nn.AdaptiveAvgPool2d(1),\n",
    "                     nn.Flatten(),\n",
    "                     nn.Linear(64,10)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = nn.Sequential(Block(3,6,5),\n",
    "                     Block(6,16,3),\n",
    "                     Block(16,16,5),\n",
    "                     Block(16,32,3),\n",
    "                     Block(32,32,5),\n",
    "                     Block(32,64,3),\n",
    "                     Block(64,64,5),\n",
    "                     nn.AdaptiveAvgPool2d(1),\n",
    "                     nn.Flatten(),\n",
    "                     nn.Linear(64,10)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = nn.Sequential(Block(3,6,5),\n",
    "                     Block(6,16,3),\n",
    "                     ResBlock(16,3),\n",
    "                     Block(16,32,3),\n",
    "                     ResBlock(32,3),\n",
    "                     Block(32,64,3),\n",
    "                     ResBlock(64,3),\n",
    "                     nn.AdaptiveAvgPool2d(1),\n",
    "                     nn.Flatten(),\n",
    "                     nn.Linear(64,10)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = nn.Sequential(Block(3,6,5),\n",
    "                     Block(6,16,3),\n",
    "                     AltBlock(16,3,5),\n",
    "                     Block(16,32,3),\n",
    "                     AltBlock(32,3,5),\n",
    "                     Block(32,64,3),\n",
    "                     AltBlock(64,3,5),\n",
    "                     nn.AdaptiveAvgPool2d(1),\n",
    "                     nn.Flatten(),\n",
    "                     nn.Linear(64,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [net0, net1, net2, net3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, criterion, epochs):\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for _ in range(epochs):\n",
    "        current = 0\n",
    "        model.train()\n",
    "        for img, lab in trainloader:\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(img.float().cuda())\n",
    "            loss = criterion(out, lab.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            current += loss.item()\n",
    "            \n",
    "        train_loss = current / len(trainloader)\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            current, acc = 0, 0\n",
    "            model.eval()\n",
    "            for img, lab in testloader:\n",
    "                out = model(img.float().cuda())\n",
    "                loss = criterion(out, lab.cuda())\n",
    "                current += loss.item() \n",
    "                _, pred = nn.Softmax(-1)(out).max(-1)\n",
    "                acc += (pred == lab.cuda()).sum().item()\n",
    "            \n",
    "            valid_loss = current / len(testloader)\n",
    "            accuracy = 100 * acc / len(testset)\n",
    "            \n",
    "        metrics.append([train_loss,valid_loss,accuracy])\n",
    "        \n",
    "    return np.array(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally get our results in a nice data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(models,epochs, lr):\n",
    "    \n",
    "    tuples = list(zip(*[3*['net0'] + 3*['net1'] + 3*['net2'] + 3*['net3'],4*['train_loss','valid_loss','accuracy']]))\n",
    "    index = pd.MultiIndex.from_tuples(tuples, names=['model', 'metric'])\n",
    "    results = pd.DataFrame(index = range(epochs),columns = index)\n",
    "    \n",
    "    for i,model in enumerate(models): \n",
    "        results[f'net{i}'] = train_loop(\n",
    "                                        model = model.cuda(),\n",
    "                                        optimizer = Adam(model.parameters(),lr=lr),\n",
    "                                        criterion = nn.CrossEntropyLoss(),\n",
    "                                        epochs = epochs\n",
    "                                       )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for simplicity, I'll use the same learning rate for all the networks but for sure you can find a better number for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th colspan=\"3\" halign=\"left\">net0</th>\n",
       "      <th colspan=\"3\" halign=\"left\">net1</th>\n",
       "      <th colspan=\"3\" halign=\"left\">net2</th>\n",
       "      <th colspan=\"3\" halign=\"left\">net3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.815294</td>\n",
       "      <td>0.877153</td>\n",
       "      <td>68.95</td>\n",
       "      <td>0.706641</td>\n",
       "      <td>0.803599</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.701039</td>\n",
       "      <td>0.810354</td>\n",
       "      <td>72.00</td>\n",
       "      <td>0.706289</td>\n",
       "      <td>0.817580</td>\n",
       "      <td>71.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.778261</td>\n",
       "      <td>0.870926</td>\n",
       "      <td>69.27</td>\n",
       "      <td>0.669520</td>\n",
       "      <td>0.811954</td>\n",
       "      <td>72.29</td>\n",
       "      <td>0.668315</td>\n",
       "      <td>0.751802</td>\n",
       "      <td>73.47</td>\n",
       "      <td>0.667333</td>\n",
       "      <td>0.792607</td>\n",
       "      <td>72.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.754965</td>\n",
       "      <td>0.860525</td>\n",
       "      <td>69.85</td>\n",
       "      <td>0.649788</td>\n",
       "      <td>0.796883</td>\n",
       "      <td>72.50</td>\n",
       "      <td>0.646536</td>\n",
       "      <td>0.739079</td>\n",
       "      <td>74.42</td>\n",
       "      <td>0.639201</td>\n",
       "      <td>0.803139</td>\n",
       "      <td>72.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.731481</td>\n",
       "      <td>0.887357</td>\n",
       "      <td>69.49</td>\n",
       "      <td>0.629868</td>\n",
       "      <td>0.782971</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.627556</td>\n",
       "      <td>0.724185</td>\n",
       "      <td>74.59</td>\n",
       "      <td>0.617718</td>\n",
       "      <td>0.791666</td>\n",
       "      <td>72.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.709845</td>\n",
       "      <td>0.830125</td>\n",
       "      <td>71.51</td>\n",
       "      <td>0.610913</td>\n",
       "      <td>0.815773</td>\n",
       "      <td>72.32</td>\n",
       "      <td>0.609816</td>\n",
       "      <td>0.721938</td>\n",
       "      <td>74.89</td>\n",
       "      <td>0.597628</td>\n",
       "      <td>0.801043</td>\n",
       "      <td>73.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.688633</td>\n",
       "      <td>0.831742</td>\n",
       "      <td>70.94</td>\n",
       "      <td>0.594478</td>\n",
       "      <td>0.772093</td>\n",
       "      <td>73.22</td>\n",
       "      <td>0.591058</td>\n",
       "      <td>0.747659</td>\n",
       "      <td>74.64</td>\n",
       "      <td>0.578132</td>\n",
       "      <td>0.757841</td>\n",
       "      <td>73.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.671347</td>\n",
       "      <td>0.857270</td>\n",
       "      <td>70.58</td>\n",
       "      <td>0.577780</td>\n",
       "      <td>0.767924</td>\n",
       "      <td>73.69</td>\n",
       "      <td>0.576838</td>\n",
       "      <td>0.781387</td>\n",
       "      <td>73.05</td>\n",
       "      <td>0.558304</td>\n",
       "      <td>0.772843</td>\n",
       "      <td>74.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model        net0                           net1                      \\\n",
       "metric train_loss valid_loss accuracy train_loss valid_loss accuracy   \n",
       "0        0.815294   0.877153    68.95   0.706641   0.803599    72.00   \n",
       "1        0.778261   0.870926    69.27   0.669520   0.811954    72.29   \n",
       "2        0.754965   0.860525    69.85   0.649788   0.796883    72.50   \n",
       "3        0.731481   0.887357    69.49   0.629868   0.782971    72.99   \n",
       "4        0.709845   0.830125    71.51   0.610913   0.815773    72.32   \n",
       "5        0.688633   0.831742    70.94   0.594478   0.772093    73.22   \n",
       "6        0.671347   0.857270    70.58   0.577780   0.767924    73.69   \n",
       "\n",
       "model        net2                           net3                      \n",
       "metric train_loss valid_loss accuracy train_loss valid_loss accuracy  \n",
       "0        0.701039   0.810354    72.00   0.706289   0.817580    71.92  \n",
       "1        0.668315   0.751802    73.47   0.667333   0.792607    72.33  \n",
       "2        0.646536   0.739079    74.42   0.639201   0.803139    72.36  \n",
       "3        0.627556   0.724185    74.59   0.617718   0.791666    72.85  \n",
       "4        0.609816   0.721938    74.89   0.597628   0.801043    73.02  \n",
       "5        0.591058   0.747659    74.64   0.578132   0.757841    73.64  \n",
       "6        0.576838   0.781387    73.05   0.558304   0.772843    74.09  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_results(models = models, epochs = 7, lr = 3e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, net2 and net3 got slightly better results than the first two, and this is just for little networks. The og paper shows the huge improvement you can get using this technique on deep networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
